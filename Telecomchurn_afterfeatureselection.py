# -*- coding: utf-8 -*-
"""Telecomchurn_IBM_afterfeaturesel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VtJYjUID6ITJLlmiNSHn5G7e47wHUDxr

Importing required packages
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import xgboost as xgb
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import tree
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from imblearn.over_sampling import SMOTE  # imblearn library can be installed using pip install imblearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
import plotly.graph_objs as go
import plotly .offline as offline
import plotly.figure_factory as ff
from sklearn.svm import SVC 
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

#pip install xgboost

"""Importing the datset and analysing"""

dataset = pd.read_csv("/content/WA_Fn-UseC_-Telco-Customer-Churn.csv")
pd.set_option('display.max_columns', None) # Will ensure that all columns are displayed
print(dataset.head())
print(dataset.shape)
print(dataset.info())
print(dataset.describe())

"""Checking for Null values"""

dataset.isnull().sum()

"""Printing all the column names"""

dataset.columns

"""Dropping Customer Id(identity) feature as it is not useful"""

dataset = dataset.drop('customerID', axis=1)
dataset.head()

"""Data Exploration - Checking churn,Senior citizen ,Internet Service, Phone service features"""

dataset.loc[:, 'Churn'].value_counts()

dataset.loc[:, 'SeniorCitizen'].value_counts()

dataset.loc[:, 'InternetService'].value_counts()

dataset.loc[:, 'PhoneService'].value_counts()

"""Plotting correalation heatmap of numeric features"""

plt.figure()
Corr=dataset[dataset.columns].corr()
sns.heatmap(Corr,annot=True)

"""converting the non-numeric data into numeric data"""

# converting the non-numeric data into numeric data.
from sklearn.preprocessing import LabelEncoder
dataset1 = dataset.apply(lambda x: LabelEncoder().fit_transform(x) if x.dtype == 'object' else x)
dataset1.head()

"""Plotting heat map for all the feature to check Correlation"""

plt.figure(figsize =(20,20))
Corr=dataset1[dataset1.columns].corr()
sns.heatmap(Corr,annot=True)

dataset1.describe()

"""Dropping the features with low feature importance obtained from Random Forest feature importance list from previous run(Before feature selection)"""

dataset1 = dataset1.drop('StreamingMovies', axis=1)
dataset1 = dataset1.drop('PhoneService', axis=1)

dataset1 = dataset1.drop('DeviceProtection', axis=1)
dataset1 = dataset1.drop('Dependents', axis=1)
dataset1 = dataset1.drop('SeniorCitizen', axis=1)

dataset1 = dataset1.drop('Partner', axis=1)
dataset1 = dataset1.drop('StreamingTV', axis=1)

"""Deviding the dataset into feature and taget label"""

# Dividing dataset into label and feature sets
X = dataset1.drop(['Churn'], axis = 1) # Features
Y = dataset1['Churn'] # Labels
print(type(X))
print(type(Y))
print(X.shape)
print(Y.shape)

"""Normalizing numerical features"""

# Normalizing numerical features so that each feature has mean 0 and variance 1
feature_scaler = StandardScaler()
X_scaled = feature_scaler.fit_transform(X)

"""Dividing the datset into Train and Test sets"""

# Dividing dataset into training and test sets
X_train, X_test, Y_train, Y_test = train_test_split( X_scaled, Y, test_size = 0.3, random_state = 100)

"""Checking the shape to check number of columns and instances"""

print(X_train.shape)
print(X_test.shape)

"""Implementing SMOTE for oversampling"""

# Implementing Oversampling to balance the dataset; SMOTE stands for Synthetic Minority Oversampling TEchnique
print("Number of observations in each class before oversampling (training data): \n", pd.Series(Y_train).value_counts())
smote = SMOTE(random_state = 101)
X_train,Y_train = smote.fit_sample(X_train,Y_train)

print("Number of observations in each class after oversampling (training data): \n", pd.Series(Y_train).value_counts())

"""Random Forest classifier implementation and its Grid search input values"""

rfc = RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1)
grid_param = {'n_estimators': [450,500, 550, 600, 650, 700, 750, 800,850,900]}

gd_sr = GridSearchCV(estimator=rfc, param_grid=grid_param, scoring='recall', cv=5)

"""Fitting the model"""

gd_sr.fit(X_train, Y_train)

"""Printing Best Paramters"""

best_parameters = gd_sr.best_params_
print(best_parameters)

best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator
print(best_result)
print(type(best_result))

"""Running the RF model for optimal parameter obtained"""

rfc = RandomForestClassifier(n_estimators=700, criterion='entropy', max_features='auto', random_state=1)
rfc.fit(X_train,Y_train)
rfc_pred = rfc.predict(X_test)
print(confusion_matrix(Y_test, rfc_pred))
print('Accuracy score:',accuracy_score(Y_test, rfc_pred))
print(classification_report(Y_test, rfc_pred))

"""Printing feature importance graph"""

featimp = pd.Series(rfc.feature_importances_, index=list(X)).sort_values(ascending=False)
print(featimp)

"""Plotting RF Confusion Matrix"""

Y_pred = rfc.predict(X_test)
conf_mat = metrics.confusion_matrix(Y_test, Y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(conf_mat,annot=True)
plt.title("Confusion_matrix")
plt.xlabel("Predicted Class")
plt.ylabel("Actual class")
plt.show()
print('Confusion matrix: \n', conf_mat)
print('TP: ', conf_mat[1,1])
print('TN: ', conf_mat[0,0])
print('FP: ', conf_mat[0,1])
print('FN: ', conf_mat[1,0])

"""SVC Classifier Implementation and defining Grid Search Hyperparamters"""

from sklearn.svm import SVC 
svc = SVC(random_state=1)
grid_param = {'C': [0.1, 1, 10, 25]}
gd_sr_sv = GridSearchCV(estimator=svc, param_grid=grid_param, scoring='recall', cv=5)

"""Fitting the model"""

gd_sr_sv.fit(X_train, Y_train)

"""Printing SVC Best Parameters"""

best_parameters_svc = gd_sr_sv.best_params_
print(best_parameters_svc)

best_result = gd_sr_sv.best_score_ # Mean cross-validated score of the best_estimator
print(best_result)
print(type(best_result))

"""Running the SVC Model for optimal values from Grid Search"""

svc = SVC(C=25, random_state=1)

"""Printing Accuracy, Confusion matrix and Classification Report"""

svc.fit(X_train,Y_train)
svc_pred = svc.predict(X_test)
print(confusion_matrix(Y_test,svc_pred))
print('accuracy_score:',accuracy_score(Y_test, svc_pred))
print(classification_report(Y_test, svc_pred))

"""plotting SVC Confusion Matrix"""

svc_pred = svc.predict(X_test)
conf_mat = metrics.confusion_matrix(Y_test, svc_pred)
plt.figure(figsize=(8,6))
sns.heatmap(conf_mat,annot=True)
plt.title("Confusion_matrix")
plt.xlabel("Predicted Class")
plt.ylabel("Actual class")
plt.show()
print('Confusion matrix: \n', conf_mat)
print('TP: ', conf_mat[1,1])
print('TN: ', conf_mat[0,0])
print('FP: ', conf_mat[0,1])
print('FN: ', conf_mat[1,0])

"""Running KNN classifier"""

classifier = KNeighborsClassifier(n_neighbors=5, metric = 'minkowski', p = 2)
classifier.fit(X_train, Y_train)

"""Fitting KNN Classifier"""

yk_pred = classifier.predict(X_test)

"""Printing KNN Evaluation metrics"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(confusion_matrix(Y_test, yk_pred))
print(classification_report(Y_test, yk_pred))
accuracy_score(Y_test, yk_pred)

"""Plotting KNN Confusion Matrix"""

yk_pred = classifier.predict(X_test)
conf_mat = metrics.confusion_matrix(Y_test, yk_pred)
plt.figure(figsize=(8,6))
sns.heatmap(conf_mat,annot=True)
plt.title("Confusion_matrix")
plt.xlabel("Predicted Class")
plt.ylabel("Actual class")
plt.show()
print('Confusion matrix: \n', conf_mat)
print('TP: ', conf_mat[1,1])
print('TN: ', conf_mat[0,0])
print('FP: ', conf_mat[0,1])
print('FN: ', conf_mat[1,0])

"""XG Boost Classiifer implementation and Providing Grid Search Values"""

xg_class=xgb.XGBClassifier(random_state=1)
xggrid_param = {'n_estimators': [300,350,400, 450 ,500, 550, 600, 650, 700],'learning_rate': [0.05,0.01,0.1, 0.5, 1]}

xggd_sr = GridSearchCV(estimator=xg_class, param_grid=xggrid_param, scoring='recall', cv=5)

xggd_sr.fit(X_train, Y_train)

"""Printing XGBoost Best Paramters"""

best_parameters = xggd_sr.best_params_
print(best_parameters)

best_result = xggd_sr.best_score_ # Mean cross-validated score of the best_estimator
print(best_result)
print(type(best_result))

"""Printing XGBoost Evaluation Metrics"""

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
xg_class = xgb.XGBClassifier(n_estimators=400, learning_rate=0.01, random_state=1)
xg_class.fit(X_train,Y_train)
xgb_pred = xg_class.predict(X_test)
print(confusion_matrix(Y_test,xgb_pred))
print('Accuracy score:',accuracy_score(Y_test, xgb_pred))
print(classification_report(Y_test, xgb_pred))

"""Plotting XGBoost Confusion Matrix"""

xgb_pred = xg_class.predict(X_test)
conf_mat = metrics.confusion_matrix(Y_test, xgb_pred)
plt.figure(figsize=(8,6))
sns.heatmap(conf_mat,annot=True)
plt.title("Confusion_matrix")
plt.xlabel("Predicted Class")
plt.ylabel("Actual class")
plt.show()
print('Confusion matrix: \n', conf_mat)
print('TP: ', conf_mat[1,1])
print('TN: ', conf_mat[0,0])
print('FP: ', conf_mat[0,1])
print('FN: ', conf_mat[1,0])

"""Importing Packages for ANN"""

import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LeakyReLU, PReLU, ELU
from keras.layers import Dropout

#pip install keras

#pip install tensorflow

ann = tf.keras.models.Sequential()

# Adding the input layer and the first hidden layer
ann.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu', input_dim = 16))

# Adding the second hidden layer
ann.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu'))

# Adding the output layer
ann.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))

# Compiling the ANN
ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['Recall','accuracy','AUC'])

"""Fitting ANN with by providing epoch and batch sizes"""

ann.fit(X_train, Y_train, batch_size = 8, epochs = 50)

"""ANN prediction on test set"""

yann_pred = ann.predict(X_test)
yannb_pred = (yann_pred > 0.5)
print(np.concatenate((yannb_pred.reshape(len(yannb_pred),1), Y_test.values.reshape(len(Y_test),1)),1))

"""Printing ANN evaluation metrics"""

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
cm = confusion_matrix(Y_test, yannb_pred)
print(cm)
print("accuracy: %f" % accuracy_score(Y_test, yannb_pred))
print(classification_report(Y_test, yannb_pred))

"""Printing ANN Confusion matrix"""

yann_pred = ann.predict(X_test)
conf_mat = metrics.confusion_matrix(Y_test, yannb_pred)
plt.figure(figsize=(8,6))
sns.heatmap(conf_mat,annot=True)
plt.title("Confusion_matrix")
plt.xlabel("Predicted Class")
plt.ylabel("Actual class")
plt.show()
print('Confusion matrix: \n', conf_mat)
print('TP: ', conf_mat[1,1])
print('TN: ', conf_mat[0,0])
print('FP: ', conf_mat[0,1])
print('FN: ', conf_mat[1,0])